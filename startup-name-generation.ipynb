{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ALL_LETTERS = string.ascii_letters + '#'\n",
    "N_LETTERS = len(ALL_LETTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(word):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', word)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )\n",
    "\n",
    "def clean_text(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text) # remove numbers\n",
    "    text = re.sub(r' +', ' ', text) #  remove more than one space\n",
    "    return text\n",
    "\n",
    "def inputTensor(word):\n",
    "    letter_indexes = [ALL_LETTERS.find(word[li]) for li in range(len(word))]\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def targetTensor(word):\n",
    "    letter_indexes = [ALL_LETTERS.find(word[li]) for li in range(1, len(word))]\n",
    "    letter_indexes.append(N_LETTERS-1)\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "def randomTrainingExample(words):\n",
    "    word = words[random.randint(0, len(words) - 1)]\n",
    "    input_tensor = inputTensor(word)\n",
    "    target_tensor = targetTensor(word)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greek myphology wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_myth_wordlist = []\n",
    "with open(\"data/startup-name-generation/greek_names.txt\", 'r', encoding='utf-8') as f:\n",
    "    greek_myth_wordlist = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Greek wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_wordlist = []\n",
    "with open(\"data/startup-name-generation/greek.txt\", 'r', encoding='utf-8') as f:\n",
    "    greek_wordlist = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gallic wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gallic_wordlist = []\n",
    "with open(\"data/startup-name-generation/gallic.txt\", 'r', encoding='utf-8') as f:\n",
    "    gallic_wordlist = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latin wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_wordlist = []\n",
    "with open(\"data/startup-name-generation/latin.txt\", 'r', encoding='utf-8') as f:\n",
    "    latin_wordlist = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum_wordlist = []\n",
    "with open(\"data/startup-name-generation/lorem-ipsum.txt\", 'r', encoding='utf-8') as f:\n",
    "    lorem_ipsum_wordlist = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and vocab preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wordlists = greek_myth_wordlist+greek_wordlist+gallic_wordlist+lorem_ipsum_wordlist\n",
    "all_wordlists = [word for item in all_wordlists for word in item.strip().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abderus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abrota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acallaris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words\n",
       "0       Abas\n",
       "1    Abderus\n",
       "2       Abia\n",
       "3     Abrota\n",
       "4  Acallaris"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=all_wordlists, columns=['words'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = df['words'].map(clean_text)\n",
    "df['words'] = df['words'].map(unicodeToAscii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicates in the data\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df.duplicated().sum()} duplicates in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['words'].apply(lambda x: len(x))\n",
    "df = df[df['length'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4215 word samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {df.shape[0]} word samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([51,  4, 20, 18])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputTensor(\"Zeus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 20, 18, 52])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetTensor(\"Zeus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMGenerator(nn.Module):\n",
    "\n",
    "#     def __init__(self, \n",
    "#                  embedding_dim, \n",
    "#                  hidden_dim, \n",
    "#                  vocab_size, \n",
    "#                  tagset_size, \n",
    "#                  bidirectional=True,\n",
    "#                  dp_rate=0.5):\n",
    "#         super(LSTMGenerator, self).__init__()\n",
    "        \n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.dp_rate = dp_rate\n",
    "#         self.bidirectional = bidirectional\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "#                             hidden_size=hidden_dim,\n",
    "#                             dropout=self.dp_rate,\n",
    "#                             bidirectional=self.bidirectional)\n",
    "\n",
    "#         self.hidden2output = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "#     def forward(self, char_ids):\n",
    "#         embeds = self.word_embeddings(char_ids)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(char_ids), 1, -1))\n",
    "#         outputs = self.hidden2output(lstm_out.view(len(char_ids), -1))\n",
    "#         outputs_scores = F.log_softmax(outputs, dim=1)\n",
    "#         return outputs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim, \n",
    "                 hidden_size,\n",
    "                 vocab_size,     \n",
    "                 num_classes,\n",
    "                 num_layers=1,\n",
    "                 bidirectional=False,\n",
    "                 dp_rate=0.5):\n",
    "        \n",
    "        super(LSTMGenerator, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if num_layers == 1:\n",
    "            self.dp_rate = 0.0\n",
    "        else:\n",
    "            self.dp_rate = dp_rate\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=self.dp_rate,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(in_features=2*hidden_size, out_features=num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # dim(x) = (seq_len)\n",
    "        # dim(embeds) = (batch, seq_len, input_size)\n",
    "\n",
    "        embeds = self.word_embeddings(x)\n",
    "        embeds = embeds.unsqueeze(0) # add batch dim\n",
    "        \n",
    "        # dim(h_0) = (num_layers *num_directions, batch, hidden_size)\n",
    "        if self.bidirectional:\n",
    "            h_0 = Variable(torch.zeros(self.num_layers*2, embeds.size(0), self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers*2, embeds.size(0), self.hidden_size))\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, embeds.size(0), self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, embeds.size(0), self.hidden_size))\n",
    "            \n",
    "        o, _ = self.lstm(embeds, (h_0, c_0))\n",
    "        \n",
    "#         if self.bidirectional:\n",
    "#             h_out = h_out.view(self.num_layers, 2, embeds.size(0), self.hidden_size)\n",
    "            # taking last hidden state\n",
    "            # dim(h_out) = (num_directions, batch, hidden_size)\n",
    "#             h_out = h_out[-1]\n",
    "\n",
    "        # in bidectional case we sum vectors over num_directions\n",
    "        # in the case of multi-layer LSTM we sum over num_layers direction\n",
    "        # dim(h_out) = (batch, hidden_size)\n",
    "#         h_out = h_out.sum(dim=0)\n",
    "\n",
    "        out = self.fc(o.view(len(x), -1))\n",
    "        out_scores = F.log_softmax(out, dim=1)\n",
    "        return out_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([33,  4, 11,  8, 14, 18]), tensor([ 4, 11,  8, 14, 18, 52]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomTrainingExample(df['words'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def train(data, generator_model, optimizer, loss_func, **kwargs):\n",
    "    n_iters = kwargs.get(\"n_iters\", 10000)\n",
    "    print_every = kwargs.get(\"print_every\", 1000)\n",
    "    plot_every = kwargs.get(\"plot_every\", 1000)\n",
    "    num_samples_to_gen = kwargs.get(\"num_samples_to_gen\", 5) \n",
    "    max_seq_length = kwargs.get(\"max_seq_length\", 10) \n",
    "\n",
    "    all_losses = []\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1, n_iters):\n",
    "        generator_model.train()\n",
    "\n",
    "        input_seq, target_seq = randomTrainingExample(data)\n",
    "\n",
    "        model.zero_grad()\n",
    "        target_scores = model(input_seq)\n",
    "\n",
    "        loss = loss_func(target_scores, target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), epoch, epoch / n_iters * 100, loss))\n",
    "            print(generate_n_sequences(num_samples_to_gen, generator_model, max_seq_length))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0\n",
    "            \n",
    "    return all_losses\n",
    "\n",
    "def generate_sequence(generator_model, max_length=20):\n",
    "    word_length = random.randint(3, max_length)\n",
    "    \n",
    "    start_letter= ALL_LETTERS[random.randint(0, len(ALL_LETTERS)-2)]\n",
    "    \n",
    "    generator_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input = inputTensor(start_letter)\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(word_length):\n",
    "            target_scores = generator_model(input)\n",
    "            idx = target_scores.argmax(dim=1).cpu().numpy()[0]\n",
    "  \n",
    "            if idx == N_LETTERS-1: # meet end token\n",
    "                break\n",
    "            else:\n",
    "                letter = ALL_LETTERS[idx]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "    return output_name.capitalize()\n",
    "\n",
    "def generate_n_sequences(n, generator_model, max_seq_length):\n",
    "    for _ in range(n):\n",
    "        print(generate_sequence(generator_model, max_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Layer 1-Directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (1000 1%) 1.9309\n",
      "Pororororo\n",
      "Froro\n",
      "Rororororo\n",
      "Sero\n",
      "Narororor\n",
      "None\n",
      "0m 3s (2000 2%) 2.8865\n",
      "Wrioriorior\n",
      "Riorio\n",
      "Uriorio\n",
      "Corio\n",
      "Nelariorio\n",
      "None\n",
      "0m 4s (3000 3%) 1.9297\n",
      "Pelarelarel\n",
      "Larelarel\n",
      "Onarelare\n",
      "Ymarel\n",
      "Urelarel\n",
      "None\n",
      "0m 6s (4000 4%) 2.1470\n",
      "Kane\n",
      "Dianer\n",
      "Querer\n",
      "Herere\n",
      "Merererere\n",
      "None\n",
      "0m 7s (5000 5%) 2.4887\n",
      "Nelanela\n",
      "Dianela\n",
      "Janela\n",
      "Phel\n",
      "Canelanel\n",
      "None\n",
      "0m 9s (6000 6%) 2.1642\n",
      "Zelananana\n",
      "Selanananan\n",
      "Zelana\n",
      "Urinananan\n",
      "Inanananan\n",
      "None\n",
      "0m 10s (7000 7%) 2.2183\n",
      "Terererer\n",
      "Xerererere\n",
      "Jarerer\n",
      "Yerererere\n",
      "Irerere\n",
      "None\n",
      "0m 12s (8000 8%) 1.7873\n",
      "Finana\n",
      "Lanana\n",
      "Penanan\n",
      "Brena\n",
      "Torenananan\n",
      "None\n",
      "0m 14s (9000 9%) 1.7016\n",
      "Uetoretor\n",
      "Banetoretor\n",
      "Setoretoret\n",
      "Metoretore\n",
      "Ianetoret\n",
      "None\n",
      "0m 15s (10000 10%) 1.7159\n",
      "Inelauelaue\n",
      "Rhauela\n",
      "Torine\n",
      "Urinelau\n",
      "Zela\n",
      "None\n",
      "0m 17s (11000 11%) 1.5525\n",
      "Maro\n",
      "Xeseseseses\n",
      "Arororo\n",
      "Vesesese\n",
      "Xeses\n",
      "None\n",
      "0m 18s (12000 12%) 1.8237\n",
      "Wuegegegege\n",
      "Dima\n",
      "Vegegeg\n",
      "Xegegege\n",
      "Kegegegege\n",
      "None\n",
      "0m 20s (13000 13%) 1.7242\n",
      "Gauegauega\n",
      "Nauegaue\n",
      "Quegauegau\n",
      "Vegauegau\n",
      "Hegaueg\n",
      "None\n",
      "0m 21s (14000 14%) 2.3564\n",
      "Tegani\n",
      "Tegan\n",
      "Keganim\n",
      "Egan\n",
      "Imanimanim\n",
      "None\n",
      "0m 23s (15000 15%) 1.5034\n",
      "Jani\n",
      "Oror\n",
      "Duelanitela\n",
      "Nitelanite\n",
      "Arororo\n",
      "None\n",
      "0m 24s (16000 16%) 2.0196\n",
      "Jane\n",
      "Lanelan\n",
      "Cane\n",
      "Wuelanel\n",
      "Melanelanel\n",
      "None\n",
      "0m 26s (17000 17%) 2.2924\n",
      "Janananana\n",
      "Senanan\n",
      "Ymanan\n",
      "Kororororo\n",
      "Menananana\n",
      "None\n",
      "0m 28s (18000 18%) 1.8168\n",
      "Janenen\n",
      "Xanenene\n",
      "Phane\n",
      "Juenenene\n",
      "Quen\n",
      "None\n",
      "0m 29s (19000 19%) 2.2430\n",
      "Yelaimaimai\n",
      "Quel\n",
      "Telaimaimai\n",
      "Ymaimaimaim\n",
      "Iaimaima\n",
      "None\n",
      "0m 31s (20000 20%) 1.6913\n",
      "Hanoue\n",
      "Fanoue\n",
      "Yeganoueg\n",
      "Queganoueg\n",
      "Keganoueg\n",
      "None\n",
      "0m 32s (21000 21%) 2.2440\n",
      "Arinelane\n",
      "Arinelan\n",
      "Inelanelane\n",
      "Wuelanelan\n",
      "Zonelan\n",
      "None\n",
      "0m 34s (22000 22%) 3.0906\n",
      "Ariselanola\n",
      "Xelano\n",
      "Bano\n",
      "Anolan\n",
      "Pela\n",
      "None\n",
      "0m 36s (23000 23%) 1.7009\n",
      "Iselanelane\n",
      "Diselanela\n",
      "Selanelanel\n",
      "Xanela\n",
      "Gane\n",
      "None\n",
      "0m 37s (24000 24%) 2.7575\n",
      "Jola\n",
      "Nelarimar\n",
      "Delari\n",
      "Erimar\n",
      "Brimarimar\n",
      "None\n",
      "0m 39s (25000 25%) 1.5799\n",
      "Fane\n",
      "Canel\n",
      "Pelanelane\n",
      "Euel\n",
      "Relanelan\n",
      "None\n",
      "0m 40s (26000 26%) 1.4354\n",
      "Hegege\n",
      "Tegege\n",
      "Wuro\n",
      "Hegeg\n",
      "Jononon\n",
      "None\n",
      "0m 42s (27000 27%) 1.5963\n",
      "Vegegege\n",
      "Gegegegeg\n",
      "Negeg\n",
      "Aninininini\n",
      "Xegeg\n",
      "None\n",
      "0m 43s (28000 28%) 1.5941\n",
      "Quelololol\n",
      "Selol\n",
      "Melolololo\n",
      "Brololo\n",
      "Uelolol\n",
      "None\n",
      "0m 45s (29000 28%) 2.0030\n",
      "Fauela\n",
      "Brolauel\n",
      "Delau\n",
      "Fuelau\n",
      "Iauelauel\n",
      "None\n",
      "0m 46s (30000 30%) 1.2618\n",
      "Laueguegue\n",
      "Naueguegue\n",
      "Dimaueg\n",
      "Imauegu\n",
      "Queguegueg\n",
      "None\n",
      "0m 48s (31000 31%) 1.8431\n",
      "Erolauegaue\n",
      "Xolauegaueg\n",
      "Xolaueg\n",
      "Egauegaueg\n",
      "Xauegau\n",
      "None\n",
      "0m 50s (32000 32%) 1.0950\n",
      "Orin\n",
      "Rinononon\n",
      "Urinonon\n",
      "Fauelauel\n",
      "Velauelaue\n",
      "None\n",
      "0m 51s (33000 33%) 1.2753\n",
      "Xoranimanim\n",
      "Ianimaniman\n",
      "Rani\n",
      "Began\n",
      "Janimani\n",
      "None\n",
      "0m 53s (34000 34%) 1.4797\n",
      "Elanelanel\n",
      "Imanelanela\n",
      "Welanelanel\n",
      "Olanelanela\n",
      "Uelanelan\n",
      "None\n",
      "0m 54s (35000 35%) 1.9373\n",
      "Rinola\n",
      "Lelalalal\n",
      "Delalala\n",
      "Yelalalal\n",
      "Uelalal\n",
      "None\n",
      "0m 56s (36000 36%) 1.6985\n",
      "Halalal\n",
      "Lalala\n",
      "Falalala\n",
      "Selala\n",
      "Imalala\n",
      "None\n",
      "0m 57s (37000 37%) 0.9467\n",
      "Velauelau\n",
      "Jolauelauel\n",
      "Kola\n",
      "Grolau\n",
      "Faue\n",
      "None\n",
      "0m 59s (38000 38%) 0.8656\n",
      "Falalal\n",
      "Xalalalal\n",
      "Gegegegegeg\n",
      "Calalalala\n",
      "Lalalala\n",
      "None\n",
      "1m 1s (39000 39%) 1.5192\n",
      "Zekeke\n",
      "Zolan\n",
      "Rinini\n",
      "Sekekekekek\n",
      "Inininin\n",
      "None\n",
      "1m 2s (40000 40%) 1.4864\n",
      "Wine\n",
      "Senen\n",
      "Inene\n",
      "Than\n",
      "Fuen\n",
      "None\n",
      "1m 4s (41000 41%) 1.8340\n",
      "Bega\n",
      "Reganegane\n",
      "Oror\n",
      "Hyneganega\n",
      "Segan\n",
      "None\n",
      "1m 5s (42000 42%) 0.8652\n",
      "Segarimari\n",
      "Hynegari\n",
      "Arima\n",
      "Segarimar\n",
      "Pori\n",
      "None\n",
      "1m 7s (43000 43%) 1.6352\n",
      "Olegalegal\n",
      "Imalegale\n",
      "Vegal\n",
      "Arimal\n",
      "Malegalegal\n",
      "None\n",
      "1m 8s (44000 44%) 1.4072\n",
      "Hekeke\n",
      "Maninin\n",
      "Manin\n",
      "Lanininin\n",
      "Ninini\n",
      "None\n",
      "1m 10s (45000 45%) 1.8504\n",
      "Inegal\n",
      "Furinega\n",
      "Palalalala\n",
      "Calalalala\n",
      "Rega\n",
      "None\n",
      "1m 11s (46000 46%) 1.2058\n",
      "Wexsex\n",
      "Zolauexs\n",
      "Xauexsex\n",
      "Zexsexsex\n",
      "Auexsexsex\n",
      "None\n",
      "1m 13s (47000 47%) 1.4426\n",
      "Weueueue\n",
      "Prinolanola\n",
      "Hanol\n",
      "Seueu\n",
      "Prinolanol\n",
      "None\n",
      "1m 15s (48000 48%) 1.2399\n",
      "Juelani\n",
      "Zelani\n",
      "Xanimaniman\n",
      "Banimanima\n",
      "Quelanima\n",
      "None\n",
      "1m 16s (49000 49%) 1.3730\n",
      "Ganimanim\n",
      "Colan\n",
      "Jolanim\n",
      "Inimaniman\n",
      "Ylani\n",
      "None\n",
      "1m 18s (50000 50%) 1.8315\n",
      "Hanininin\n",
      "Ueseseseses\n",
      "Ninininini\n",
      "Nininininin\n",
      "Ninininin\n",
      "None\n",
      "1m 19s (51000 51%) 1.3751\n",
      "Inolalalal\n",
      "Dinol\n",
      "Jues\n",
      "Wino\n",
      "Weseseseses\n",
      "None\n",
      "1m 21s (52000 52%) 2.3864\n",
      "Manori\n",
      "Winor\n",
      "Orinorin\n",
      "Segegege\n",
      "Khano\n",
      "None\n",
      "1m 22s (53000 53%) 1.9916\n",
      "Corin\n",
      "Bexse\n",
      "Zorinori\n",
      "Duex\n",
      "Lanorinor\n",
      "None\n",
      "1m 24s (54000 54%) 1.8560\n",
      "Jorininini\n",
      "Corininini\n",
      "Inininin\n",
      "Rhan\n",
      "Hanininini\n",
      "None\n",
      "1m 25s (55000 55%) 2.0505\n",
      "Keueue\n",
      "Xanininin\n",
      "Veueueu\n",
      "Lani\n",
      "Canini\n",
      "None\n",
      "1m 27s (56000 56%) 2.0758\n",
      "Ylaneg\n",
      "Vegrinegr\n",
      "Negrinegr\n",
      "Quegri\n",
      "Jonegrine\n",
      "None\n",
      "1m 28s (57000 56%) 1.6905\n",
      "Cato\n",
      "Tolato\n",
      "Zolatolatol\n",
      "Satolatola\n",
      "Wico\n",
      "None\n",
      "1m 30s (58000 57%) 1.2424\n",
      "Oricolanexs\n",
      "Ganexs\n",
      "Jolan\n",
      "Ianexs\n",
      "Pexse\n",
      "None\n",
      "1m 32s (59000 59%) 0.8164\n",
      "Ricola\n",
      "Blanexsex\n",
      "Exsex\n",
      "Quexsexsex\n",
      "Zexs\n",
      "None\n",
      "1m 33s (60000 60%) 1.6361\n",
      "Quelauelaue\n",
      "Lauelauel\n",
      "Nininin\n",
      "Iauelauel\n",
      "Hauelauel\n",
      "None\n",
      "1m 35s (61000 61%) 0.8318\n",
      "Nanexon\n",
      "Inexonexone\n",
      "Orhanexon\n",
      "Dexone\n",
      "Ylanexone\n",
      "None\n",
      "1m 36s (62000 62%) 1.1637\n",
      "Delarin\n",
      "Zelarinela\n",
      "Narine\n",
      "Uxsela\n",
      "Bari\n",
      "None\n",
      "1m 38s (63000 63%) 1.0001\n",
      "Canononon\n",
      "Kanon\n",
      "Xseueueue\n",
      "Kanonono\n",
      "Ianononono\n",
      "None\n",
      "1m 39s (64000 64%) 1.9657\n",
      "Lexsexse\n",
      "Bainexse\n",
      "Dexs\n",
      "Uxsex\n",
      "Nexsex\n",
      "None\n",
      "1m 41s (65000 65%) 1.3983\n",
      "Ininini\n",
      "Zonininini\n",
      "Urinin\n",
      "Urininini\n",
      "Deueu\n",
      "None\n",
      "1m 42s (66000 66%) 1.5625\n",
      "Gain\n",
      "Fuerininini\n",
      "Cainininin\n",
      "Kerin\n",
      "Phainini\n",
      "None\n",
      "1m 44s (67000 67%) 1.6554\n",
      "Colaicolaic\n",
      "Ricolai\n",
      "Haico\n",
      "Maicolaico\n",
      "Ericolaico\n",
      "None\n",
      "1m 45s (68000 68%) 1.3043\n",
      "Geri\n",
      "Derinola\n",
      "Ylanolanol\n",
      "Manolanol\n",
      "Kolanolan\n",
      "None\n",
      "1m 47s (69000 69%) 1.5054\n",
      "Zelonon\n",
      "Rono\n",
      "Melonononon\n",
      "Brono\n",
      "Cononon\n",
      "None\n",
      "1m 49s (70000 70%) 1.6425\n",
      "Coneui\n",
      "Poneuineuin\n",
      "Xauineuineu\n",
      "Orin\n",
      "Arine\n",
      "None\n",
      "1m 50s (71000 71%) 0.9410\n",
      "Maue\n",
      "Winini\n",
      "Xoninin\n",
      "Vegauegau\n",
      "Mauegaueg\n",
      "None\n",
      "1m 52s (72000 72%) 1.7485\n",
      "Xsegan\n",
      "Ueganininin\n",
      "Ganin\n",
      "Eganininini\n",
      "Seganini\n",
      "None\n",
      "1m 53s (73000 73%) 1.3335\n",
      "Reronini\n",
      "Veronini\n",
      "Konin\n",
      "Deroni\n",
      "Paueronini\n",
      "None\n",
      "1m 55s (74000 74%) 0.7697\n",
      "Kori\n",
      "Gainelai\n",
      "Winelai\n",
      "Gain\n",
      "Xsel\n",
      "None\n",
      "1m 57s (75000 75%) 1.1268\n",
      "Hapeueueue\n",
      "Fueueu\n",
      "Wineueu\n",
      "Zeueueueu\n",
      "Seueue\n",
      "None\n",
      "1m 58s (76000 76%) 0.8604\n",
      "Weueueu\n",
      "Gaueu\n",
      "Queueu\n",
      "Veueueue\n",
      "Weueueue\n",
      "None\n",
      "2m 0s (77000 77%) 1.0526\n",
      "Reuineuineu\n",
      "Auineui\n",
      "Auineuineui\n",
      "Rine\n",
      "Arineuineu\n",
      "None\n",
      "2m 1s (78000 78%) 1.1871\n",
      "Moninininin\n",
      "Velaininini\n",
      "Uinininini\n",
      "Hainininin\n",
      "Finin\n",
      "None\n",
      "2m 3s (79000 79%) 1.1597\n",
      "Yninini\n",
      "Zelan\n",
      "Uranin\n",
      "Caninini\n",
      "Branini\n",
      "None\n",
      "2m 4s (80000 80%) 1.9982\n",
      "Qurineur\n",
      "Haur\n",
      "Beuri\n",
      "Qurin\n",
      "Leurin\n",
      "None\n",
      "2m 6s (81000 81%) 1.7050\n",
      "Laueu\n",
      "Gaueueue\n",
      "Toninini\n",
      "Caueu\n",
      "Phaue\n",
      "None\n",
      "2m 8s (82000 82%) 1.1941\n",
      "Winexo\n",
      "Ianexon\n",
      "Hanexonexo\n",
      "Canex\n",
      "Hanexon\n",
      "None\n",
      "2m 9s (83000 83%) 1.3086\n",
      "Eueueue\n",
      "Tonono\n",
      "Urinononon\n",
      "Ynonononono\n",
      "Fueueueueue\n",
      "None\n",
      "2m 11s (84000 84%) 1.3027\n",
      "Zauelau\n",
      "Gauelauel\n",
      "Gauelauel\n",
      "Zelauelau\n",
      "Prinelau\n",
      "None\n",
      "2m 12s (85000 85%) 2.3703\n",
      "Seue\n",
      "Gani\n",
      "Zonininin\n",
      "Haninin\n",
      "Nininini\n",
      "None\n",
      "2m 14s (86000 86%) 1.1396\n",
      "Anol\n",
      "Tega\n",
      "Ganola\n",
      "Meganol\n",
      "Seganolano\n",
      "None\n",
      "2m 16s (87000 87%) 1.6493\n",
      "Nelanela\n",
      "Anela\n",
      "Rhanelan\n",
      "Zanelane\n",
      "Rinela\n",
      "None\n",
      "2m 17s (88000 88%) 1.6595\n",
      "Weuin\n",
      "Garineu\n",
      "Carineuine\n",
      "Quineuin\n",
      "Beui\n",
      "None\n",
      "2m 19s (89000 89%) 0.6056\n",
      "Zanourinour\n",
      "Banourinour\n",
      "Kourino\n",
      "Degan\n",
      "Brino\n",
      "None\n",
      "2m 20s (90000 90%) 0.8307\n",
      "Yeuri\n",
      "Ononon\n",
      "Jurinono\n",
      "Ylanon\n",
      "Eurinono\n",
      "None\n",
      "2m 22s (91000 91%) 1.2254\n",
      "Ylinininin\n",
      "Fanininini\n",
      "Dini\n",
      "Juinininini\n",
      "Xseganin\n",
      "None\n",
      "2m 23s (92000 92%) 1.6560\n",
      "Zeui\n",
      "Uineuineui\n",
      "Euineuineu\n",
      "Marineu\n",
      "Pharineu\n",
      "None\n",
      "2m 25s (93000 93%) 1.9214\n",
      "Juexsexsex\n",
      "Xainini\n",
      "Ainininini\n",
      "Zain\n",
      "Haininini\n",
      "None\n",
      "2m 26s (94000 94%) 1.8482\n",
      "Dexse\n",
      "Fexsexs\n",
      "Gexsexse\n",
      "Gain\n",
      "Iainononono\n",
      "None\n",
      "2m 28s (95000 95%) 0.7293\n",
      "Besesese\n",
      "Quesese\n",
      "Winin\n",
      "Jonininin\n",
      "Leseses\n",
      "None\n",
      "2m 29s (96000 96%) 1.0433\n",
      "Reseseses\n",
      "Vesese\n",
      "Nesesese\n",
      "Nesesese\n",
      "Bainesese\n",
      "None\n",
      "2m 31s (97000 97%) 1.0841\n",
      "Lainex\n",
      "Zexs\n",
      "Mainexsexs\n",
      "Fuexsexsex\n",
      "Onexsexsexs\n",
      "None\n",
      "2m 32s (98000 98%) 1.5594\n",
      "Querininini\n",
      "Perin\n",
      "Serininin\n",
      "Canini\n",
      "Brin\n",
      "None\n",
      "2m 34s (99000 99%) 0.9427\n",
      "Arineurine\n",
      "Eurineu\n",
      "Seurin\n",
      "Iaineur\n",
      "Urin\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "params = {\"n_iters\": 100000,\n",
    "          \"print_every\": 1000,\n",
    "          \"plot_every\": 1000,\n",
    "          \"num_samples_to_gen\": 5,\n",
    "          \"max_seq_length\": 10\n",
    "}\n",
    "\n",
    "model = LSTMGenerator(EMBEDDING_DIM, HIDDEN_DIM, N_LETTERS, N_LETTERS)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "all_losses = train(df['words'].values, model, optimizer, loss_function, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Layer 1-Directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 17s (1000 1%) 1.9586\n",
      "Here\n",
      "Uererer\n",
      "Wererer\n",
      "Sererererer\n",
      "Verererere\n",
      "None\n",
      "2m 36s (2000 2%) 2.5024\n",
      "Sere\n",
      "Kerere\n",
      "Gere\n",
      "Verere\n",
      "Nererererer\n",
      "None\n",
      "3m 54s (3000 3%) 3.8917\n",
      "Derererer\n",
      "Anerererer\n",
      "Erere\n",
      "Gane\n",
      "Jerererer\n",
      "None\n",
      "5m 11s (4000 4%) 1.8449\n",
      "Ferere\n",
      "Lerererer\n",
      "Xerere\n",
      "Lere\n",
      "Derererer\n",
      "None\n",
      "6m 35s (5000 5%) 2.9536\n",
      "Daro\n",
      "Erororororo\n",
      "Eroror\n",
      "Caro\n",
      "Qeror\n",
      "None\n",
      "7m 55s (6000 6%) 1.7361\n",
      "Hererere\n",
      "Jarerer\n",
      "Onere\n",
      "Serer\n",
      "Xere\n",
      "None\n",
      "9m 15s (7000 7%) 1.9671\n",
      "Xeraneran\n",
      "Veraneran\n",
      "Qaner\n",
      "Oran\n",
      "Qaneran\n",
      "None\n",
      "10m 30s (8000 8%) 2.4595\n",
      "Herererere\n",
      "Nerere\n",
      "Corerererer\n",
      "Qererererer\n",
      "Urerererer\n",
      "None\n",
      "11m 49s (9000 9%) 1.4321\n",
      "Tererere\n",
      "Ferer\n",
      "Brererere\n",
      "Nerererer\n",
      "Bererer\n",
      "None\n",
      "13m 0s (10000 10%) 1.9680\n",
      "Euelanel\n",
      "Qelan\n",
      "Canelanel\n",
      "Canelanelan\n",
      "Telanel\n",
      "None\n",
      "14m 12s (11000 11%) 2.2605\n",
      "Relanelanel\n",
      "Elanel\n",
      "Orelanela\n",
      "Janela\n",
      "Delanelan\n",
      "None\n",
      "15m 24s (12000 12%) 1.6947\n",
      "Wanelan\n",
      "Ianelane\n",
      "Belanel\n",
      "Belanelanel\n",
      "Orel\n",
      "None\n",
      "16m 36s (13000 13%) 2.0125\n",
      "Fuenen\n",
      "Urisuenene\n",
      "Penen\n",
      "Urisu\n",
      "Senenenenen\n",
      "None\n",
      "17m 48s (14000 14%) 2.3737\n",
      "Xenenene\n",
      "Tenenenene\n",
      "Nenenenene\n",
      "Fanenenenen\n",
      "Janenenenen\n",
      "None\n",
      "19m 2s (15000 15%) 2.5251\n",
      "Nela\n",
      "Qelane\n",
      "Fanelanela\n",
      "Zela\n",
      "Onelanelane\n",
      "None\n",
      "20m 15s (16000 16%) 2.1784\n",
      "Iaren\n",
      "Ypenenen\n",
      "Rene\n",
      "Renenenene\n",
      "Venenenene\n",
      "None\n",
      "21m 32s (17000 17%) 1.7952\n",
      "Henen\n",
      "Qenenenenen\n",
      "Banenenen\n",
      "Manen\n",
      "Then\n",
      "None\n",
      "22m 49s (18000 18%) 2.2254\n",
      "Rererere\n",
      "Onererer\n",
      "Marer\n",
      "Dise\n",
      "Harererer\n",
      "None\n",
      "24m 6s (19000 19%) 1.8440\n",
      "Iananananan\n",
      "Lananana\n",
      "Iananana\n",
      "Yenanananan\n",
      "Wanan\n",
      "None\n",
      "25m 25s (20000 20%) 2.1837\n",
      "Renenenene\n",
      "Venene\n",
      "Vene\n",
      "Uene\n",
      "Ianen\n",
      "None\n",
      "26m 42s (21000 21%) 1.1265\n",
      "Penenen\n",
      "Qanenenenen\n",
      "Inenenenene\n",
      "Nenen\n",
      "Ganenen\n",
      "None\n",
      "28m 3s (22000 22%) 2.7030\n",
      "Disenenenen\n",
      "Senenenen\n",
      "Zonenenene\n",
      "Denene\n",
      "Xanenenenen\n",
      "None\n",
      "29m 23s (23000 23%) 1.6818\n",
      "Canenene\n",
      "Epenenenene\n",
      "Inenenenen\n",
      "Canen\n",
      "Xenenenene\n",
      "None\n",
      "30m 44s (24000 24%) 1.5971\n",
      "Onise\n",
      "Veniseni\n",
      "Menisenis\n",
      "Janisenisen\n",
      "Venisenise\n",
      "None\n",
      "32m 5s (25000 25%) 1.4071\n",
      "Hypo\n",
      "Ianela\n",
      "Nelane\n",
      "Uela\n",
      "Tone\n",
      "None\n",
      "33m 25s (26000 26%) 2.9019\n",
      "Yenenenenen\n",
      "Nenenenen\n",
      "Fuenen\n",
      "Uenene\n",
      "Wenenenenen\n",
      "None\n",
      "34m 46s (27000 27%) 1.3695\n",
      "Janinin\n",
      "Quen\n",
      "Kenini\n",
      "Waninin\n",
      "Meninini\n",
      "None\n",
      "36m 10s (28000 28%) 2.5470\n",
      "Senianiani\n",
      "Veniania\n",
      "Ganianian\n",
      "Denia\n",
      "Nianianian\n",
      "None\n",
      "37m 36s (29000 28%) 1.6404\n",
      "Vena\n",
      "Benanananan\n",
      "Lenananan\n",
      "Menanananan\n",
      "Janana\n",
      "None\n",
      "39m 0s (30000 30%) 1.6463\n",
      "Beninini\n",
      "Rhenininini\n",
      "Ganinini\n",
      "Rinin\n",
      "Yenin\n",
      "None\n",
      "40m 25s (31000 31%) 1.2278\n",
      "Leninini\n",
      "Inin\n",
      "Dueninin\n",
      "Quenininin\n",
      "Penini\n",
      "None\n",
      "41m 53s (32000 32%) 1.7715\n",
      "Yponinin\n",
      "Weninini\n",
      "Hypo\n",
      "Caninini\n",
      "Anini\n",
      "None\n",
      "43m 24s (33000 33%) 1.9843\n",
      "Yponinini\n",
      "Seninini\n",
      "Nininin\n",
      "Cani\n",
      "Yponin\n",
      "None\n",
      "44m 52s (34000 34%) 2.0643\n",
      "Uenene\n",
      "Lenene\n",
      "Wanenenene\n",
      "Nenene\n",
      "Wenenene\n",
      "None\n",
      "46m 24s (35000 35%) 1.9578\n",
      "Wenono\n",
      "Ganon\n",
      "Lano\n",
      "Venon\n",
      "Rinonon\n",
      "None\n",
      "47m 55s (36000 36%) 1.7559\n",
      "Wanonon\n",
      "Rino\n",
      "Delanonon\n",
      "Wanononono\n",
      "Orinonono\n",
      "None\n",
      "49m 25s (37000 37%) 2.3607\n",
      "Rinela\n",
      "Kelanel\n",
      "Fanelanelan\n",
      "Melane\n",
      "Ianelanel\n",
      "None\n",
      "50m 56s (38000 38%) 1.6383\n",
      "Conerinerin\n",
      "Orineri\n",
      "Inerinerine\n",
      "Meriner\n",
      "Qiner\n",
      "None\n",
      "52m 28s (39000 39%) 1.2290\n",
      "Serinono\n",
      "Lanonono\n",
      "Mano\n",
      "Serinonon\n",
      "Kano\n",
      "None\n",
      "54m 3s (40000 40%) 1.0329\n",
      "Hanin\n",
      "Enin\n",
      "Ueninini\n",
      "Ininininin\n",
      "Penin\n",
      "None\n",
      "55m 34s (41000 41%) 1.7017\n",
      "Xineganeg\n",
      "Queganeg\n",
      "Jegane\n",
      "Kegan\n",
      "Ianeg\n",
      "None\n",
      "57m 10s (42000 42%) 2.0376\n",
      "Hane\n",
      "Zeganeg\n",
      "Leganegan\n",
      "Teganeganeg\n",
      "Leganega\n",
      "None\n",
      "58m 45s (43000 43%) 1.4329\n",
      "Serine\n",
      "Verineriner\n",
      "Yeri\n",
      "Haner\n",
      "Alane\n",
      "None\n",
      "60m 21s (44000 44%) 1.3018\n",
      "Xerinerine\n",
      "Manerine\n",
      "Perine\n",
      "Verin\n",
      "Iuerinerin\n",
      "None\n",
      "61m 56s (45000 45%) 2.5100\n",
      "Canerineri\n",
      "Kaner\n",
      "Anerinerin\n",
      "Zeriner\n",
      "Janer\n",
      "None\n",
      "63m 31s (46000 46%) 1.9716\n",
      "Fanon\n",
      "Ianonono\n",
      "Ianonon\n",
      "Seueueueueu\n",
      "Bano\n",
      "None\n",
      "65m 8s (47000 47%) 1.7400\n",
      "Wanerin\n",
      "Ypone\n",
      "Ponerine\n",
      "Ganeri\n",
      "Kaner\n",
      "None\n",
      "66m 48s (48000 48%) 0.9717\n",
      "Zone\n",
      "Nenene\n",
      "Euenenenen\n",
      "Ganene\n",
      "Kenen\n",
      "None\n",
      "68m 28s (49000 49%) 1.1488\n",
      "Gane\n",
      "Banerinerin\n",
      "Erin\n",
      "Zerine\n",
      "Xeri\n",
      "None\n",
      "70m 6s (50000 50%) 3.2880\n",
      "Fanerin\n",
      "Kane\n",
      "Neri\n",
      "Fuerinerine\n",
      "Zerine\n",
      "None\n",
      "71m 46s (51000 51%) 2.2112\n",
      "Xanerin\n",
      "Kaneriner\n",
      "Orin\n",
      "Canerineri\n",
      "Verineriner\n",
      "None\n",
      "73m 28s (52000 52%) 1.4304\n",
      "Yerinin\n",
      "Zeri\n",
      "Lani\n",
      "Conini\n",
      "Conininini\n",
      "None\n",
      "75m 9s (53000 53%) 1.2875\n",
      "Oninini\n",
      "Zonin\n",
      "Ganin\n",
      "Urinini\n",
      "Yerinin\n",
      "None\n",
      "76m 52s (54000 54%) 2.9317\n",
      "Laneri\n",
      "Banerine\n",
      "Xerineriner\n",
      "Neriner\n",
      "Werinerine\n",
      "None\n",
      "78m 39s (55000 55%) 1.6533\n",
      "Phaneri\n",
      "Rhanerineri\n",
      "Rhanerin\n",
      "Erineriner\n",
      "Aner\n",
      "None\n",
      "80m 27s (56000 56%) 1.8746\n",
      "Haninininin\n",
      "Nininin\n",
      "Seninin\n",
      "Phani\n",
      "Reninini\n",
      "None\n",
      "82m 12s (57000 56%) 1.3322\n",
      "Gainininini\n",
      "Yerini\n",
      "Perininini\n",
      "Seri\n",
      "Veri\n",
      "None\n",
      "84m 0s (58000 57%) 2.1707\n",
      "Xineriner\n",
      "Merinerine\n",
      "Kerin\n",
      "Fuerine\n",
      "Meri\n",
      "None\n",
      "85m 49s (59000 59%) 1.7983\n",
      "Dini\n",
      "Euerininini\n",
      "Ganinin\n",
      "Ganininini\n",
      "Uerininin\n",
      "None\n",
      "87m 37s (60000 60%) 1.2686\n",
      "Maininin\n",
      "Terinini\n",
      "Xini\n",
      "Toninininin\n",
      "Kerinin\n",
      "None\n",
      "89m 27s (61000 61%) 1.8931\n",
      "Onininini\n",
      "Gainin\n",
      "Zeue\n",
      "Wininin\n",
      "Thaininini\n",
      "None\n",
      "91m 18s (62000 62%) 1.7410\n",
      "Fuinini\n",
      "Terininin\n",
      "Luinin\n",
      "Inin\n",
      "Perininini\n",
      "None\n",
      "93m 10s (63000 63%) 1.7465\n",
      "Jainininini\n",
      "Yeri\n",
      "Werinin\n",
      "Onini\n",
      "Jerinini\n",
      "None\n",
      "95m 2s (64000 64%) 2.4137\n",
      "Derinin\n",
      "Beri\n",
      "Cani\n",
      "Urinininini\n",
      "Banininin\n",
      "None\n",
      "96m 54s (65000 65%) 1.6005\n",
      "Orinonono\n",
      "Urino\n",
      "Nerinononon\n",
      "Herinon\n",
      "Manononon\n",
      "None\n",
      "98m 46s (66000 66%) 1.4211\n",
      "Thanininini\n",
      "Eponinini\n",
      "Velaninin\n",
      "Yela\n",
      "Quoni\n",
      "None\n",
      "100m 39s (67000 67%) 2.0713\n",
      "Peueueueu\n",
      "Lanononon\n",
      "Rinono\n",
      "Banonononon\n",
      "Ianonono\n",
      "None\n",
      "102m 30s (68000 68%) 0.9727\n",
      "Hydinononon\n",
      "Conon\n",
      "Erinonono\n",
      "Werino\n",
      "Fuerinono\n",
      "None\n",
      "104m 28s (69000 69%) 2.3270\n",
      "Xserinerine\n",
      "Hanerinerin\n",
      "Janerinerin\n",
      "Terine\n",
      "Reri\n",
      "None\n",
      "106m 26s (70000 70%) 1.6917\n",
      "Kanono\n",
      "Serin\n",
      "Ganonononon\n",
      "Herinononon\n",
      "Xano\n",
      "None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6979d4bbe117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mall_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-e859ded31642>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, generator_model, optimizer, loss_func, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n"
     ]
    }
   ],
   "source": [
    "model = LSTMGenerator(EMBEDDING_DIM, HIDDEN_DIM, N_LETTERS, N_LETTERS, num_layers=2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "all_losses = train(df['words'].values, model, optimizer, loss_function, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Layer Bi-Directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMGenerator(EMBEDDING_DIM, HIDDEN_DIM, N_LETTERS, N_LETTERS, num_layers=2, bidirectional=True)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "all_losses = train(df['words'].values, model, optimizer, loss_function, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/caesarlupum/gov-names\n",
    "\n",
    "https://www.kaggle.com/dattapiy/sec-edgar-companies-list\n",
    "\n",
    "https://www.kaggle.com/dalreada/all-uk-active-company-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
